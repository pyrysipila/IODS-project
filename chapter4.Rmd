---
title: "RStudio Exercise 4"
author: "Pyry SipilÃ¤"
date: "23 November 2018"
output: 
    html_document:
      toc: TRUE
---
  
#Chapter 4: Clustering and classification
  
####Analysis exercises
  
####1
A new markdown file has been created.
  
####2
  
```{r}
  library(MASS)
  data("Boston")
  str(Boston)
  dim(Boston)
  colnames(Boston)
```
Boston is a classic dataset describing living conditions such as urban environment, location, crime rate, air quality and economic issues in towns in the suburbs of Boston. It has 506 observations and 14 variables. More information on the data can be found here: https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html
  
####3
```{r}
#install.packages("tidyverse")
  library(tidyverse)
  #install.packages("corrplot")
  library(corrplot)
  #histogram
  Boston %>%
    gather() %>% 
    ggplot(aes(value)) +
      facet_wrap(~ key, scales = "free") +
      geom_histogram()
  #scatter plot of all variables in the dataset
  plot(Boston)
  # calculate the correlation matrix,round it and print it
  cor_matrix<-cor(Boston)
  cor_matrix <- cor_matrix %>% round(digits=2)
  cor_matrix
  # visualize the correlation matrix
  corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos ="d",tl.cex=0.6)
  
```
Many variables have skewed distributions. Rm (average number of rooms per dwelling) has a nice distribution close to normal.
  
There are quite a few strong correlations. The strongest positive correlation is between tax (full-value property-tax rate per $10,000) and rad (index of accessibility to radial highways) (0.91) and the strongest negative correlation is between dis (weighted mean of distances to five Boston employment centres) and nox (nitrogen oxides concentration (parts per 10 million)) (-0.77).
  
####4
```{r}
  # center and standardize variables
  boston_scaled <- scale(Boston)
  
  # summaries of the scaled variables
  summary(boston_scaled)
  
  # class of the boston_scaled object
  class(boston_scaled)
  
  # change the object to data frame
  boston_scaled <- as.data.frame(boston_scaled)
  
  # create a quantile vector of crim and print it
  bins <- quantile(boston_scaled$crim)
  bins
  
  # create a categorical variable 'crime'
  crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=c("low","med_low","med_high","high"))
  
  # look at the table of the new factor crime
  table(crime)
  
  # remove original crim from the dataset
  boston_scaled <- dplyr::select(boston_scaled, -crim)
  
  # add the new categorical variable to scaled data
  boston_scaled <- data.frame(boston_scaled, crime)
  
  # number of rows in the Boston dataset 
  n <- nrow(Boston)
  
  # choose randomly 80% of the rows
  ind <- sample(n,  size = n * 0.8)
  
  # create train set
  train <- boston_scaled[ind,]
  
  # create test set 
  test <- boston_scaled[-ind,]
  
```
All the scaled variables have mean zero (and standard deviation 1).
  
####5
  
```{r}
  # linear discriminant analysis
  lda.fit <- lda(crime ~ ., data = train)
  
  # print the lda.fit object
  lda.fit
  
  # the function for lda biplot arrows
  lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
    heads <- coef(x)
    arrows(x0 = 0, y0 = 0, 
           x1 = myscale * heads[,choices[1]], 
           y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
    text(myscale * heads[,choices], labels = row.names(heads), 
         cex = tex, col=color, pos=3)
  }
  
  # target classes as numeric
  classes <- as.numeric(train$crime)
  
  # plot the lda results
  plot(lda.fit, dimen = 2, col = classes, pch = classes)
  lda.arrows(lda.fit)
```
  
####6
  
```{r}
  # save the correct classes from test data
  correct_classes <- test$crime
  
  # remove the crime variable from test data
  test <- dplyr::select(test, -crime)
  
  # predict classes with test data
  lda.pred <- predict(lda.fit, newdata = test)
  
  # cross tabulate the results
  table(correct = correct_classes, predicted = lda.pred$class)
```
The proportion of correct predictions is 14/27 (52%) for low crime rate, 20/30 (67%) for med_low crime rate, 16/22 (73%) for med_high crime rate and 23/23 (100%) for high crime rate. Overall, the predictions are quite good (proportion of correct predictions 73/102~72%), and the higher the crime rate the better the predictions.
  
####7

```{r}
  #Reload and scale the Boston data
  data("Boston")
  boston_scaled <- scale(Boston)
  
  set.seed(123)
  # euclidean distance matrix
  dist_eu <- dist(boston_scaled)
  # look at the summary of the distances
  summary(dist_eu)
  
  # k-means clustering
  km <-kmeans(boston_scaled, centers = 3)
  #visualize it
  pairs(Boston, col = km$cluster)
  
  ##investigate the optimal number of clusters
  #set seed for reproducibility
  set.seed(123)
  
  # determine the number of clusters
  k_max <- 10
  
  # calculate the total within sum of squares
  twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})
  
  # visualize the results
  qplot(x = 1:k_max, y = twcss, geom = 'line')
  #2 seems to be an optimal number of clusters, because the total within sum of squares drops dramatically when moving from one cluster to two clusters.
  
  #run the algorithm again
  km <-kmeans(boston_scaled, centers = 2)
  
  # plot the Boston dataset with clusters
  pairs(boston_scaled, col = km$cluster)
```
  
Two is the optimal number of clusters. Two clusters seem to differentiate well between towns of low and high crime rate. Other variables do not seem to be able to clearly differentiate between the clusters.
  
####Bonus
```{r}
  #Relaod and scale the Boston data
  data("Boston")
  boston_scaled <- scale(Boston)
  #change the data to a data frame
  boston_scaled <- as.data.frame(boston_scaled)
  
  # k-means clustering
  km <-kmeans(boston_scaled, centers = 3)
  
  #add the clusters to the data
  boston_scaled <- data.frame(boston_scaled, km$cluster)
  glimpse(boston_scaled)
  #perform LDA using the clusters as target classes
  lda.fit <- lda(km.cluster ~ ., data = boston_scaled)
  
  # the function for lda biplot arrows
  lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
    heads <- coef(x)
    arrows(x0 = 0, y0 = 0, 
           x1 = myscale * heads[,choices[1]], 
           y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
    text(myscale * heads[,choices], labels = row.names(heads), 
         cex = tex, col=color, pos=3)
  }
  # plot the lda results
  plot(lda.fit, dimen = 2, col = classes, pch = classes)
  lda.arrows(lda.fit)
```
Rad (index of accessibility to radial highways is the most influential linear separator for the clusters. Other influental linear separators are tax (full-value property-tax rate per $10,000), age (proportion of owner-occupied units built prior to 1940) and dis (weighted mean of distances to five Boston employment centres).
  
####Super bonus
```{r}
  lda.fit <- lda(crime ~ ., data = train)
  
  model_predictors <- dplyr::select(train, -crime)
  
  # check the dimensions
  dim(model_predictors)
  dim(lda.fit$scaling)
  
  # matrix multiplication
  matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
  matrix_product <- as.data.frame(matrix_product)
  
  #install.packages("plotly")
  library(plotly)
  
  plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = train$crime)
  
  train2 <- train
  train2$num_crime <- as.numeric(train2$crime)
  train2$num_crime <- scale(train2$num_crime)
  train2$crime <- train2$num_crime
  train2 <- select(train2,-num_crime)
  km <- kmeans(train2, centers = 2)
  
  plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km$cluster)
  
```
There is an obvious difference: crime has four classes but kmeans only two clusters. Apart that, The plots look surprisingly similar. The clusters formed by k-means clustering seem to mostly differentiate between the towns with high crime rate vs the towns with lower crime rates.